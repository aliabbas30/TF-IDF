{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ddd2d8",
   "metadata": {},
   "source": [
    "## TF-IDF from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68e7653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['jumper.', 'good', 'jumps', 'the', 'quick', 'over', 'lazy', 'dog.', 'is', 'fox', 'a', 'brown']\n",
      "TF: [0.05882353 0.05882353 0.05882353 0.17647059 0.11764706 0.05882353\n",
      " 0.05882353 0.05882353 0.05882353 0.11764706 0.05882353 0.11764706]\n",
      "DF: [1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "IDF: [2.14006616 2.14006616 2.14006616 2.14006616 2.14006616 2.14006616\n",
      " 2.14006616 2.14006616 2.14006616 2.14006616 2.14006616 2.14006616]\n",
      "TF-IDF: [0.12588624 0.12588624 0.12588624 0.37765873 0.25177249 0.12588624\n",
      " 0.12588624 0.12588624 0.12588624 0.25177249 0.12588624 0.25177249]\n"
     ]
    }
   ],
   "source": [
    "'''Import Numpy'''\n",
    "import numpy as np\n",
    "\n",
    "''''' Example document '''''\n",
    "document = \"The quick brown fox jumps over the lazy dog. The quick brown fox is a good jumper.\"\n",
    "\n",
    "'''Split document into individual words\n",
    "'''\n",
    "words = document.lower().split()\n",
    "\n",
    "\n",
    "'''Getting vocabolary'''\n",
    "unique_words = list(set(words))\n",
    "\n",
    "'''Count number of occurrences of each word in document\n",
    "'''\n",
    "word_counts = [words.count(word) for word in unique_words]\n",
    "\n",
    "'''Calculate term frequency (TF)\n",
    "'''\n",
    "tf = np.array(word_counts) / len(words)\n",
    "\n",
    "'''Calculate document frequency (DF)\n",
    "'''\n",
    "df = np.array([np.sum([word in d.lower().split() for d in [document]]) for word in unique_words])\n",
    "\n",
    "''' Calculate inverse document frequency (IDF)\n",
    "'''\n",
    "idf = np.log(len(words) / (1 + df))\n",
    "\n",
    "''' Calculate TF-IDF\n",
    "'''\n",
    "tf_idf = tf * idf\n",
    "\n",
    "# Print results\n",
    "print(\"Words:\", unique_words)\n",
    "print(\"TF:\", tf)\n",
    "print(\"DF:\", df)\n",
    "print(\"IDF:\", idf)\n",
    "print(\"TF-IDF:\", tf_idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee94543b",
   "metadata": {},
   "source": [
    "## TF-IDF using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f84488a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.42796959 0.         0.34989318 0.         0.67049706\n",
      "  0.34989318 0.         0.34989318]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the corpus\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Create the TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# Transform the corpus into a TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.transform(corpus)\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea4e4a",
   "metadata": {},
   "source": [
    "## TF-IDF using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5ff7803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5614561943922499), (1, 0.5614561943922499), (2, 0.5614561943922499), (3, 0.23302537487517574)]\n",
      "[(0, 0.40251125658964493), (1, 0.40251125658964493), (3, 0.16705726536655122), (5, 0.8050225131792899)]\n",
      "[(3, 0.11435555192640605), (6, 0.551061299883055), (7, 0.551061299883055), (8, 0.551061299883055), (9, 0.2755306499415275)]\n",
      "[(2, 0.31622776601683794), (9, 0.31622776601683794), (10, 0.6324555320336759), (11, 0.6324555320336759)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Define the corpus\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenized_corpus = [doc.split() for doc in corpus]\n",
    "\n",
    "# Create the dictionary from the tokenized corpus\n",
    "dictionary = corpora.Dictionary(tokenized_corpus)\n",
    "\n",
    "# Create the bag-of-words corpus\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_corpus]\n",
    "\n",
    "# Create the TF-IDF model\n",
    "tfidf_model = TfidfModel(bow_corpus)\n",
    "\n",
    "# Calculate the TF-IDF scores for the corpus\n",
    "tfidf_corpus = tfidf_model[bow_corpus]\n",
    "\n",
    "# Print the TF-IDF scores\n",
    "for doc in tfidf_corpus:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3edbe7",
   "metadata": {},
   "source": [
    "## Bag of words(BoW) from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "463fdb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['blue', 'sun', 'dog', 'jumps', 'lazy.', 'sky', 'bright.', 'quick', 'The', 'the', 'over', 'lazy', 'dog.', 'is', 'fox', 'and', 'brown']\n",
      "Document 1: The quick brown fox jumps over the lazy dog.\n",
      "Bag-of-words: [0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1]\n",
      "Document 2: The brown fox is quick and the blue dog is lazy.\n",
      "Bag-of-words: [1 0 1 0 1 0 0 1 1 1 0 0 0 2 1 1 1]\n",
      "Document 3: The sky is blue and the sun is bright.\n",
      "Bag-of-words: [1 1 0 0 0 1 1 0 1 1 0 0 0 2 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The brown fox is quick and the blue dog is lazy.\",\n",
    "    \"The sky is blue and the sun is bright.\"\n",
    "]\n",
    "\n",
    "# Create a vocabulary of all unique words in the documents\n",
    "vocabulary = list(set(word for doc in documents for word in doc.split()))\n",
    "\n",
    "# Create a numpy array to store the bag-of-words representation of each document\n",
    "bag_of_words = np.zeros((len(documents), len(vocabulary)), dtype=np.int32)\n",
    "\n",
    "# Iterate over each document and update its bag-of-words representation\n",
    "for i, doc in enumerate(documents):\n",
    "    words = doc.split()\n",
    "    for j, word in enumerate(vocabulary):\n",
    "        count = words.count(word)\n",
    "        bag_of_words[i, j] = count\n",
    "\n",
    "# Print the vocabulary and bag-of-words representation for each document\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1}: {doc}\")\n",
    "    print(\"Bag-of-words:\", bag_of_words[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e4f60f",
   "metadata": {},
   "source": [
    "## BOW using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44af8020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and', 'blue', 'bright', 'brown', 'dog', 'fox', 'is', 'jumps', 'lazy', 'over', 'quick', 'sky', 'sun', 'the']\n",
      "Document 1: The quick brown fox jumps over the lazy dog.\n",
      "Bag-of-words: [[0 0 0 1 1 1 0 1 1 1 1 0 0 2]]\n",
      "Document 2: The brown fox is quick and the blue dog is lazy.\n",
      "Bag-of-words: [[1 1 0 1 1 1 2 0 1 0 1 0 0 2]]\n",
      "Document 3: The sky is blue and the sun is bright.\n",
      "Bag-of-words: [[1 1 1 0 0 0 2 0 0 0 0 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The brown fox is quick and the blue dog is lazy.\",\n",
    "    \"The sky is blue and the sun is bright.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents and transform the documents into a bag-of-words representation\n",
    "bag_of_words = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print the vocabulary and bag-of-words representation for each document\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names())\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1}: {doc}\")\n",
    "    print(\"Bag-of-words:\", bag_of_words[i].toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bac2dc",
   "metadata": {},
   "source": [
    "## The end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cab1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7a685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a6619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d33c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
